### [卷积神经网络](https://www.cnblogs.com/wj-1314/p/9754072.html)

​		CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络**避免了对图像的复杂前期预处理**，可以直接输入原始图像，因而得到了更为广泛的应用。

​		一般的，CNN的基本结构包括两层，其一为**特征提取层**，每个神经元的输入与前一层的**局部接受域相连，并提取该局部的特征**。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；其二是**特征映射层**，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。

#### 1 图像输入

​		如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。

​		我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。

​		![img](https://img2018.cnblogs.com/blog/1226410/201810/1226410-20181009202559906-1527391226.png)

#### 2 特征提取

　	回想前面讲到的“局部感受野”模式，对于CNN来说，它是一小块一小块地来进行比对，在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性。

![img](https://img2018.cnblogs.com/blog/1226410/201810/1226410-20181009202646989-200335159.png)

### 3 卷积

​		当给定一张新图时，CNN并不能准确地知道这些特征到底要匹配原图的哪些部分，所以它会在原图中把每一个可能的位置都进行尝试，相当于把这个feature（特征）变成了一个过滤器。这个用来匹配的过程就被称为卷积操作，这也是卷积神经网络名字的由来。
　　卷积的操作如下图所示：

![img](https://img2018.cnblogs.com/blog/1226410/201810/1226410-20181009202815705-1130979104.gif)

​		对三个特征图像不断地重复着上述过程，通过每一个feature（特征）的卷积操作，会得到一个新的二维数组，称之为feature map。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。

![img](https://static.oschina.net/uploads/space/2018/0210/003257_dZEn_876354.png)

​		可以看出，当图像尺寸增大时，其内部的加法、乘法和除法操作的次数会增加得很快，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得计算量变得相当庞大。

### 4 池化

​		为了有效地减少计算量，CNN使用的另一个有效的工具被称为“池化(Pooling)”。池化就是将输入图像进行缩小，减少像素信息，只保留重要信息。

​		       ![img](https://static.oschina.net/uploads/space/2018/0210/003329_XCiq_876354.png)

​		最大池化（max-pooling）保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。通过加入池化层，图像缩小了，能很大程度上减少计算量，降低机器负载。

### 5 激活函数

​		常用的激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者ReLU常见于卷积层。回顾一下前面讲的感知机，感知机在接收到各个输入，然后进行求和，再经过激活函数后输出。激活函数的作用是用来加入非线性因素，把卷积层输出结果做非线性映射。

​		在卷积神经网络中，激活函数一般使用ReLU(The Rectified Linear Unit，修正线性单元)，它的特点是收敛快，求梯度简单。计算公式也很简单，max(0,T)，即对于输入的负值，输出全为0，对于正值，则原样输出。
　　下面看一下本案例的ReLU激活函数操作过程：
　　第一个值，取max(0,0.77)，结果为0.77，如下图

![img](https://static.oschina.net/uploads/space/2018/0210/003349_hKBL_876354.png)

对所有的feature map执行ReLU激活函数操作，结果如下：

![img](https://static.oschina.net/uploads/space/2018/0210/003405_TmMY_876354.png)

### 6 深度神经网络

​		通过将上面所提到的卷积、激活函数、池化组合在一起，就变成下图：

![img](https://static.oschina.net/uploads/space/2018/0210/003417_g8lB_876354.png)

​		通过加大网络的深度，增加更多的层，就得到了深度神经网络，如下图：

![img](https://static.oschina.net/uploads/space/2018/0210/003423_SvCC_876354.png)

### 7 全连接层

​		全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。
　　首先将经过卷积、激活函数、池化的深度网络后的结果串起来，如下图所示：

![img](https://static.oschina.net/uploads/space/2018/0210/003434_MygV_876354.png)

​		由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重（如预测字母X的所有连接的权重）

![img](https://static.oschina.net/uploads/space/2018/0210/003440_MLD0_876354.png)

​		在利用该模型进行结果识别时，根据刚才提到的模型训练得出来的权重，以及经过前面的卷积、激活函数、池化等深度网络计算出来的结果，进行加权求和，得到各个结果的预测值，然后取值最大的作为识别的结果（如下图，最后计算出来字母X的识别值为0.92，字母O的识别值为0.51，则结果判定为X）

![img](https://static.oschina.net/uploads/space/2018/0210/003445_oQwf_876354.png)

上述这个过程定义的操作为”全连接层“(Fully connected layers)，全连接层也可以有多个，如下图：

![img](https://static.oschina.net/uploads/space/2018/0210/003451_GX0E_876354.png)

### 8 卷积神经网络

​		将以上所有结果串起来后，就形成了一个“卷积神经网络”（CNN）结构，如下图所示：

![img](https://static.oschina.net/uploads/space/2018/0210/003504_HAIT_876354.png)

​		最后，再回顾总结一下，卷积神经网络主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接层），下图便是著名的手写文字识别卷积神经网络结构图：

![img](https://static.oschina.net/uploads/space/2018/0210/003512_hpv5_876354.png)

