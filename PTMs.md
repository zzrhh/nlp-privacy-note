## 1. 预训练模型背景

### 1.1 预训练模型的优势

​		随着深度学习的发展，模型参数的数量迅速增加。需要更大的数据集来完全训练模型参数并防止过度拟合。但是，由于注释成本极其昂贵，因此对于大多数NLP任务而言，构建大规模的标记数据集是一项巨大的挑战，尤其是对于语法和语义相关的任务。
相反，大规模的未标记语料库相对容易构建。为了利用巨大的未标记文本数据，我们可以首先从它们中学习良好的表示形式，然后将这些表示形式用于其他任务。最近的研究表明，借助从大型无注释语料库中的PTMs提取的表示形式，可以在许多NLP任务上显着提高性能。

		* 在庞大的文本语料库上进行预训练可以学习通用的语言表示形式并帮助完成下游任务。
		* 预训练提供了更好的模型初始化，通常可以带来更好的泛化性能并加快目标任务的收敛速度。

* 可以将预训练视为一种正则化，以避免对小数据过度拟合。

### 1.2 预训练模型的发展过程

​		**第一代PTMs**旨在学习良好的单词嵌入。由于下游任务不再需要这些模型本身，因此对于计算效率而言它们通常很浅，例如Skip-Gram（根据单词预测上下文）和GloVe。尽管这些经过预训练的嵌入可以捕获单词的语义，但它们不受上下文限制，无法捕获上下文中的高级概念，例如多义歧义消除，句法结构，语义角色，回指。
​		**第二代PTMs**专注于学习上下文词嵌入，主要目的是通过一个预训练的编码器能够输出上下文相关的**词向量**，解决一词多义的问题。例如CoV e [120]，ELMo ，OpenAI GPT 和BERT 。仍然需要这些学习的编码器来表示下游任务在上下文中的单词。此外，还提出了各种预训练任务来学习PTMs，以用于不同的目的。

## 2. 预训练模型概述

​		在语言方面，一个好的表示法应该捕获隐藏在文本数据中的隐含语言规则和常识知识，例如词汇含义，句法结构，语义角色，甚至是语用学。
​		分布式表示的核心思想是通过低维实值向量来描述一段文本的含义。向量的每个维度都没有相应的意义，而整体代表了一个具体的概念。图1说明了NLP的通用神经体系结构。词嵌入有两种：非上下文嵌入和上下文嵌入。它们之间的区别在于，单词的嵌入是否根据出现的上下文而动态变化。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020050614363229.png)

## 3.PTMs概述

​		PTMs之间的主要区别是**上下文编码器的用法**，**预训练任务**和**目的**。我们在第2.2节中简要介绍了上下文编码器的体系结构。在本节中，我们重点介绍预训练任务，并给出PTMs的分类法。

### 3.1 预训练任务

#### 3.1.1 语言建模（LM）

​		NLP中最常见的无监督任务是概率语言建模（LM），这是一个经典的概率密度估计问题。尽管LM是一个笼统的概念，但在实践中，LM通常特别是指自回归LM或单向LM。

​		给定一个庞大的语料库，我们可以使用最大似然估计（MLE）训练整个网络。

​		单向LM的一个缺点是每个令牌的表示仅编码向左上下文令牌及其本身。但是，更好的文本上下文表示应从两个方向对上下文信息进行编码。一种改进的解决方案是双向LM（BiLM），它由两个单向LM组成：向前的从左到右的LM和向后的从右到左的LM。

#### 3.1.2屏蔽语言建模（MLM）

### 3.2 PTMs的分类

​		为了阐明NLP的现有PTMs的关系，我们建立了PTMs的分类法，从四个不同的角度对现有PTMs进行了分类：
1.表示类型：根据用于下游任务的表示，我们可以将PTMs分为非上下文和上下文楷模。
2.体系结构：PTMs使用的骨干网，包括LSTM，Transformer编码器，Transformer解码器和完整的Transformer体系结构。 “变压器”是指标准的编码器-解码器体系结构。 “ Transformer编码器”和“ Transformer解码器”分别表示标准Transformer体系结构的编码器和解码器部分。它们的区别在于，解码器部分使用带有三角矩阵的蒙版自我注意来防止令牌进入其未来（正确）位置。
3.预训练任务类型：PTMs使用的预训练任务的类型。我们已经在第3.1节中讨论了它们。
4.扩展：针对各种场景而设计的PTMs，包括知识丰富的PTMs，多语言或语言特定的PTMs，多模型PTMs，特定领域的PTMs和压缩的PTMs。我们将在第4节中特别介绍这些扩展。