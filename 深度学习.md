### 深度学习

> 机器学习的分支，人工神经网络为基础，对数据的特征进行学习的方法

### 机器学习和深度学习的区别

> * a. 特征提取
>   1. 人工的特征提取过程
>   2. 自动地提取特征
> * b. 数据量
>   1. 较少
>   2. 较多

### 场景

* 图像识别
* 自然语言
* 语音技术

### NLP 领域的核心概念

>1. **特征表示**
>
>  1.1 分布式表示
>
>  >​	可以简单理解为某些数据的向量。如用向量 ![[公式]](https://www.zhihu.com/equation?tex=v%3D%282%2C4%2C1%2C6%2C3%29) 表示“语言”这个单词。
>  >
>  >![](https://pic4.zhimg.com/80/v2-7aeeee78c44dee63bbe4d076c150fbbf_720w.jpg)
>
>  >​		这种被称为“稀疏”或“本地”的非分布式表示在很多方面是效率低下的。首先，随着我们观察的形状数量的增加，表示的维度将会增加。更重要的是，它没有给出关于这些形状之间如何相互关联的任何信息。而这是分布式表示真正的价值所在：它有通过概念来发现数据之间“语义相似性”的能力。
>  >
>  >![img](https://pic3.zhimg.com/80/v2-e514ee19ef54c04a846b0583c955c48a_720w.jpg)
>  >
>  >​		图2显示了同一组形状的分布式表示。它用与方向和形状概念相关的多个“记忆单元”表示形状的信息。“记忆单元”含有关于每个形状以及形状之间如何相互关联的信息。当通过分布式表示法（例如图3中的圆圈）来表示一个新形状时，我们不会再增加维度。而且即使我们之前没有见过圆，我们也知道关于圆的一些信息，因为它与其他形状有关。
>  >
>  >![img](https://pic2.zhimg.com/80/v2-f75ce63a90ee3dc479cb969f0ad44451_720w.jpg)
>  >
>  >​		在上面对于形状的分布式表示的例子里，我们用了四个概念或特征（垂直、水平、矩形、椭圆）来表示。在这种情况下，我们必须事先知道这些重要和显着的特征是什么。但在很多情况下，这是一件很难或不可能的事情。正因为如此，特征工程在经典机器学习技术中才变得如此重要。找到对于数据的良好表示对于分类或聚类等任务的成功至关重要。深度学习能获得巨大成功的原因之一是神经网络具有学习丰富的分布式数据表示的能力。
>  >
>  >​		在一个用LSTM做句子情感分类的任务中，我们通过创建一个映射表来记录句子中每个单词到一个整数索引的关系。单词到整数的映射是单词数据的非分布式稀疏表示。例如，buy这个单词被映射到索引值25，long这个单词被表示为索引68。值得注意的是，这个方法和一个长度为vocab_size的“one-hot编码”（索引中有1表示该单词，而其他位置都是0）向量表示方式是等价的。这两个表示法是相互独立的，尽管在它们在语义上是相似的。两种方法里的两个词之间是没有关系信息的，两种方法里的词都仅被表示为在映射里的位置。
>  >
>  >​		要想得到单词的分布式表示，可以有两种方式。一，是通过Word2Vecotr等模型学习得到。二，作为特定任务的模型训练过程的一部分，用端到端的方式学习来获得分布式表示。
>  >
>  >​		**总结**，神经网络能学习到分布式数据表示的能力是深度学习能对于许多不同类型的问题非常有效的主要原因之一。这个概念的力量和美感使得表示学习成为深度学习研究中最令人兴奋和最活跃的领域之一。
>
>
>
>2. **word2vec**
>
>  ​		Word2vec 是 Word Embedding方式之一，属于 NLP领域。他是将词转化为「可计算」「结构化」的向量的过程。这是一种基于统计方法来获得词向量的方法，他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。
>
>  ​		这种算法有2种训练模式
>
>    * 通过上下文来预测当前词（CBOW）
>    
>    * 通过当前词来预测上下文（Skip-gram）
>      优点：
>    
>       * 由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）
>       * 比之前的 Embedding方 法维度更少，所以速度更快
>       * 通用性很强，可以用在各种 NLP 任务中
>    
>      缺点：
>    
>      	* 由于词和向量是一对一的关系，所以多义词的问题无法解决
>      	* Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化
>
>  2.1 Word Embedding
>
>  ​		它就是将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。
>
>  ![word embedding：将非结构化数据转化为结构化数据](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-representation.png)
>
>  ​		文本表示的方法有很多种，下面只介绍 3 类方式：
>
>  		* 独热编码 | one-hot representation
>  		* 整数编码
>  		* 词嵌入 | word embedding
>
>  ![word embedding的关系](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-guanxi.png)
>
>  ​	2.1.1 独热编码
>
>  ![one-hot编码](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-one-hot.png)
>
>  ​		但是在实际情况中，文本中很可能出现成千上万个不同的词，这时候向量就会非常长。其中99%以上都是 0。
>
>  one-hot 的缺点如下
>
>   - 无法表达词语之间的关系
>
>   - 这种过于稀疏的向量，导致计算和存储的效率都不高
>
>     2.1.2 整数编码
>
>  ![整数编码](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-number.png)
>
>  ​		将句子里的每个词拼起来就是可以表示一句话的向量。
>
>  整数编码的缺点如下
>
>    - 无法表达词语之间的关系
>    
>    - 对于模型解释而言，整数编码可能具有挑战性。
>    
>      2.1.3 词嵌入
>    
>      词嵌入并不特指某个具体的算法，跟上面2种方式相比，这种方法有几个明显的优势：
>    
>      * 他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。
>    
>      * 语意相似的词在向量空间上也会比较相近。
>    
>      * 通用性很强，可以用在不同的任务中。
>    
>        ![word embedding：语意相似的词在向量空间上也会比较相近](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-we.png)
>
>  3. #####  Seq2Seq框架
>
>     ​		NLP应用程序中的大多数基础框架都依赖于序列到序列（seq2seq）模型，在该模型中，不仅输入而且将输出表示为序列。这些模型在各种应用中都很常见，包括**机器翻译、文本摘要、语音到文本和文本到语音**应用。
>
>     ​		最常见的seq2seq框架**由编码器和解码器组成**。**编码器提取输入数据的序列，并生成一个中级输出，随后由解码器消耗，以产生一系列最终输出**。编码器和解码器通常通过一系列递归神经网络或LSTM单元实现。
>
>     ​		编码器采用长度为T的序列，X = {x1，x2，···，xT}，其中xt∈V = {1，···，| V |}是来自词汇表的单个输入的表示V，然后生成输出状态ht。随后，解码器从编码器获取最后一个状态，即ht，并根据其当前状态st，开始生成大小为L的输出，Y’= {y’1，y’ 2，…，y’ L}，但真地输出是yt。在不同的应用中，解码器可以利用更多信息（例如上下文向量或自注意力向量）来生成更好的输出。
>
>     ![Encoder将现实问题转化为数学问题](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-encoder.png)
>
>     ![Decoder求解数学问题，并转化为现实世界的解决方案](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-decoder.png)
>
>     ![图解Encoder-Decoder](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-Encoder-Decoder.png)
>
>     关于 Encoder-Decoder，有2 点需要说明
>
>     	*  不论输入和输出的长度是什么，中间的「向量 c」 长度都是固定的
>     	*  根据不同的任务可以选择不同的编码器和解码器
>
>     Encoder-Decoder 的应用
>
>     ![Encoder-Decoder的一些应用](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-yingyong.png)
>
>     Encoder-Decoder 的缺陷
>
>     ​		Encoder（编码器）和 Decoder（解码器）之间只有一个「向量 c」来传递信息，且 c 的长度固定。将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 3000X3000 像素的图片也压缩到 100KB，看上去就模糊了。当输入信息太长时，会丢失掉一些信息。
>
>  4. **Attention 解决信息丢失问题**
>
>     Ａttention 模型的特点是 Eecoder 不再将整个输入序列编码为固定长度的中间向量 Ｃ，而是编码成一个向量的序列。引入了 *Ａttention* 的 *Encoder-Decoder* 模型如下图：
>
>     ![图解attention](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-attention.png)
>
>  5. **数据集**
>
>     基准数据集通常采用以下三种形式之一。
>       * 第一个是从各种现实世界实验获得的real-world data。
>       * 第二种是人工模拟现实世界模式的synthetic data。生成合成数据以代替实际数据供使用。此类数据集在需要的数据量远远大于可用数据量的应用程序中，或者在诸如医疗保健领域这样的隐私考虑因素至关重要且严格的应用程序中尤为重要。
>       * 第三类是toy datasets，目的是用于演示和可视化。通常，它们是人工生成的；一般不需要表示real-world data模式。
>
>     ​        深度学习利用的基础是数据的可用性，以教会系统有关**模式识别**的知识。模型的有效性取决于数据的质量。
>
>     
>

### 深度学习中的NLP任务

>1. **基本任务**
>
>  1.1  词性标注理论
>
>  ​		词性标注在本质上是分类问题，将语料库中的单词按词性分类。一个词的词性由其在所属语言的含义、形态和语法功能决定。以汉语为例，汉语的词类系统有18个子类，包括7类体词，4类谓词、5类虚词、代词和感叹词。词类不是闭合集，而是有兼词现象，例如“制服”在作为“服装”和作为“动作”时会被归入不同的词类，因此词性标注与上下文有关
>
>  1.2  应用
>
>  ​		词性标注是文本数据的**预处理**环节之一，原始文本在NLP或文本挖掘应用中，首先通过字符分割（word segmentation）和字符嵌入（word embedding）被**向量化**，随后通过**词性标注**得到高阶层特征，并输入[语法分析器](https://baike.baidu.com/item/语法分析器/10598664)执行[语义分析](https://baike.baidu.com/item/语义分析/8853372)（sentiment analysis）、[指代消解](https://baike.baidu.com/item/指代消解/7605552)（coreference resolution）等任务
>
>  1.2.1  文本向量化
>
>  ​		文本表示是自然语言处理中的基础工作，文本表示的好坏直接影响到整个自然语言处理系统的性能。文本向量化就是**将文本表示成一系列能够表达文本语义的向量**，是文本表示的一种重要方式。目前对文本向量化大部分的研究都是通过**词向量化**实现的，也有一部分研究者将**句子**作为文本处理的基本单元，于是产生了doc2vec和str2vec技术。
>
>    * 词袋模型
>    
>      ​		最早的以词语为基础处理单元的文本项量化方法。该模型产生的向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频率。
>    
>    * 词向量(word2vec)  
>    
>      ​		随着互联网技术的发展，大量无标注数据的产生，研究重心转移到利用无标注数据挖掘有价值的信息上来。词向量(word2vec)技术就是为了利用神经网络，从大量无标注的文本中提取有用的信息而产生的。
>    
>      ​		相比于词袋模型，词向量是一种更为有效的表征方式。怎么理解呢？词向量其实就是用一个一定维度(例如128，256维)的向量来表示词典里的词。
>    
>      经过训练之后的词向量，能够表征词语之间的关系。例如，“香蕉”和“苹果”之间的距离，会比“香蕉”和“茄子”之间的距离要近。
>    
>      通过多维向量表示，也能更为方便的进行计算。例如，“女人”+“漂亮” =“女神”。
>
>​	1.3 分析
>
>​	1.3.1句法分析
>
>​	是**指为句子分配句法结构**。它的任务是给定一个句子，分析出句子的短语结构句法树。例如给定句子“The little boy likes red tomatoes .”，它的成分句法树如下图所示
>
>![img](https://pic2.zhimg.com/80/v2-e273bf8c8257e989adb6184ca5cb84a5_720w.jpg)
>
>​	1.3.2 依赖关系分析
>
>​	**依存关系结构显示了目标句子中单词之间的结构关系**。在**依赖关系分析**中，短语元素和短语结构规则对过程没有帮助。而是，**仅根据句子中的单词和单词之间的关联关系来表达句子的句法结构**。
>
>​	1.3.3 语义角色标签
>
>​	语义角色标注（SRL）是文本参数的识别和分类过程。它旨在表征元素，以确定“谁”对“谁”做了“什么”以及“如何”、“在哪里”和“何时”。它识别句子的谓词-论元结构。本质上，谓词指的是“什么”，而论元则由关联的参与者和文本中的属性组成。 SRL的目标是提取谓词和相关论元之间的语义关系。
>
>2. **文本分类**
>
>   ​	**文本分类**的主要目的是**为文本部分（可以是单词、句子或整个文档）分配预定义的类别，以进行初步分类以及进一步的组织和分析**。一个简单的**例子**是将给定文档分类为政治或非政治新闻。
>
>3. **信息抽取**
>
>   ​	**信息抽取**可**从“非结构化”数据（例如社交媒体帖子和在线新闻）中识别结构化信息**。深度学习已用于有关子任务（例如**命名实体识别、关系抽取、共指消解和事件抽取**）的信息抽取。
>
>   3.1 命名实体识别
>
>   ​	**命名实体识别（NER）旨在将上下文中的命名实体定位和分类为预定义的类别**，例如人物和地点的名称。
>
>   3.2 关系抽取
>
>   ​	**关系抽取旨在查找实体对之间的语义关系**。**递归神经网络**（RNN）模型已经被提出通过学习组成向量表示来进行**语义关系分类**。对于**关系分类**，也通过**提取词汇和句子级别的特征来使用CNN体系结构**。
>
>   3.3 共指消解
>
>   ​	**共指消解包括标识引用同一实体的上下文中的提及**。例如，提到的“car”，“Camry”和“it”都可以指同一实体。当前最先进的方法利用**注意力机制**。
>
>   3.4 事件抽取
>
>   ​	**从文本中抽取信息的一种特定类型是事件**。这样的抽取可以**涉及识别与事件有关的触发词，并且将标签分配给代表事件触发的实体提及**。**卷积神经网络**已被用于事件检测。他们使用基于特征的方法处理问题，包括**详尽的特征工程和用于特征生成的错误传播现象**。
>
>4. 情感分析
>
>   ​	情感分析的主要目标是**通过上下文挖掘从文本中提取主观信息**。情感分析被认为是基于源数据的高级推理。**情感分析**有时被称为观点挖掘，因为它的**主要目标是分析有关产品、问题和各种主题的人类观点、情感甚至是对产品的情绪**。
>
>   ​	**情感分析**研究的一个关键方面是内容粒度。考虑到此标准，情感分析通常分为三个类别/级别：**文档级、句子级和方面级**。
>
>   4.1 文档级的情感分析
>
>   ​	在文档级，任务是确定整个文档是否反映出对一个实体的正面或负面情绪。这与针对多个条目的观点挖掘不同。
>
>   4.2 句子级的情感分析
>
>   ​	在句子级别，情感分析**确定句子中表达的观点的正面、负面或中立**。**句子级情感分类**的一般假设是，**表达的句子中仅存在一个持有者的观点。通过学习短语的向量空间表示，将递归自动编码器用于句子级别的情感标签预测**。
>
>   4.3 方面级的情感分析
>
>   ​		文档级和句子级的情感分析通常集中于**情感本身**，而不是**情感目标**，例如产品。在假设存在情感及其目标的情况下，方面级的情感分析直接针对一种观点。文档或句子通常可能没有正面或负面的情绪，但可能有多个不同目标的子任务，每个目标都有正面或负面的情绪。这可能使方面级的分析比其他类型的情感分类更具挑战性。
>
>5. **机器翻译**
>
>   简单来说，机器翻译就是把一种语言翻译成另外一种语言。
>
>   5.1 主要挑战。
>
>   5.1.1 译文选择
>
>   ​	在翻译一个句子的时候，会面临很多选词的问题，因为语言中一词多义的现象比较普遍。比如这个例子中，源语言句子中的『看』，可以翻译成『look』、『watch』 『read 』和 『see』等词，如果不考虑后面的宾语『书』的话，这几个译文都对。在这个句子中，只有机器翻译系统知道『看』的宾语『书』，才能做出正确的译文选择，把『看』翻译为『read』 ，『read a book』。译文选择是机器翻译面临的第一个挑战。
>
>   5.1.2 译文调序
>
>   ​	由于文化及语言发展上的差异，我们在表述的时候，有时候先说这样一个成份，后面说另外一个成份 ，但是，在另外一种语言中，这些语言成分的顺序可能是完全相反的。比如在这个例子中，『在周日』，这样一个时间状语在英语中习惯上放在句子后面。再比如，像中文和日文的翻译，中文的句法是『主谓宾』，而日文的句法是『主宾谓』，日文把动词放在句子最后。比如中文说『我吃饭』，那么日语呢就会说『我饭吃』。当句子变长时，语序调整会更加复杂。
>
>   5.1.3 数据稀疏
>
>   ​	不完全统计，现在人类的语言大约有超过五千种。现在的机器翻译技术大部分都是基于大数据的，只有在大量的数据上训练才能获得一个比较好的效果。而实际上，语言数量的分布非常不均匀的。右边的饼图显示了中文相关语言的一个分布情况，大家可以看到，百分之九十以上的都是中文和英文的双语句对，中文和其他语言的资源呢，是非常少的。在非常少的数据上，想训练一个好的系统是非常困难的。
>
>   5.2 神经机器翻译
>
>   ![img](https://pic1.zhimg.com/80/v2-de112febc7d5802b87936ba60d8e4679_720w.jpg?source=1940ef5c)
>
>   ​	神经网络翻译从模型上来说相对简单，它主要包含两个部分，一个是编码器，一个是解码器。编码器是把源语言经过一系列的神经网络的变换之后，表示成一个高维的向量。解码器负责把这个高维向量再重新解码（翻译）成目标语言。
>
>   ![img](https://pic2.zhimg.com/80/v2-93c390d5cf3aca668b8638b358c264c2_720w.jpg?source=1940ef5c)
>
>   ​	一般采用的方法是，基于n-gram（n元语法）的评价方法。通常大家都用BLEU值。一般地，BLEU是在多个句子构成的集合（测试集）上计算出来的。这个测试集可能包含一千个句子或者两千个句子，去整体上衡量机器翻译系统好还是不好。有了这个测试集以后，需要有参考答案（reference）。所谓参考答案就是人类专家给出的译文。这个过程很像考试，通过比较参考答案和系统译文的匹配程度，来给机器翻译系统打分。
>
>6. **问题回答**
>
>   ​	**问答（QA）是信息检索（IR）的细粒度版本**。在IR中，必须从一组文档中检索所需的一组信息。所需的信息可以是**特定的文档、文本、图像**等。另一方面，在质量检查中要寻求特定的答案，通常是那些可以从可用文档中推断出来。 NLP的其他领域（例如阅读理解和对话系统）与问题回答相交。
>
>7. **文档摘要**
>
>   ​	**文档摘要是指一组问题，这些问题涉及在给定一个或多个文档作为输入的情况下生成摘要语句。**
>      通常，**文本摘要**分为两类
>
>   7.1 抽取式摘要，其目的是识别文档中最突出的句子并将其作为摘要返回。
>   7.2 生成式摘要，目标是从头开始生成摘要句子；它们可能包含原始文档中未出现的新颖词。
>    这些方法中的每一种都有其自身的优点和缺点。抽取式摘要易于产生冗长的，有时是重叠的摘要句子；但是，结果反映了作者的表达方式。生成式方法产生的摘要较短，但是很难训练。
>
>8. ##### 对话系统
>
>   ​	对话系统正迅速成为人机交互的主要工具，部分原因是其具有广阔的发展潜力和商业价值。一种应用是自动化客户服务，既支持在线业务又支持实体业务。客户期望与公司及其服务打交道时，其速度、准确性和尊重度会不断提高。由于知识渊博的人力资源的高昂成本，公司经常转向智能对话机器。请注意，会话机器和对话机器通常可以互换使用。
>   8.1 基于任务的系统：基于任务的对话系统的结构通常包含以下元素
>
>   ​	•自然语言理解（NLU）：该组件通过为语音内容分配构成结构来处理理解和解释用户的语音环境（例如，一个句子）并捕获其句法表示和语义解释，以允许后端操作/任务。无论对话上下文如何，通常都会利用NLU。
>    •对话管理（DM）：由NLU生成的表示形式将由对话管理处理，对话管理将调查上下文并返回合理的语义相关响应。
>    •自然语言生成（NLG）：自然语言生成（NLG）组件根据DM组件提供的响应产生话语。
>   8.2 基于非任务的系统
>
>   ​		与**基于任务的对话系统**相反，设计和部署**基于非任务的对话系统**的目标是**使机器具有与人类进行自然对话的能力**。

