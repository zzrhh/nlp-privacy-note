### 深度学习

> 机器学习的分支，人工神经网络为基础，对数据的特征进行学习的方法

### 机器学习和深度学习的区别

> * a. 特征提取
>   1. 人工的特征提取过程
>   2. 自动地提取特征
> * b. 数据量
>   1. 较少
>   2. 较多

### 场景

* 图像识别
* 自然语言
* 语音技术

### NLP 领域的核心概念

>1. **特征表示**
>
>   1.1 分布式表示
>
>   >​	可以简单理解为某些数据的向量。如用向量 ![[公式]](https://www.zhihu.com/equation?tex=v%3D%282%2C4%2C1%2C6%2C3%29) 表示“语言”这个单词。
>   >
>   >![](https://pic4.zhimg.com/80/v2-7aeeee78c44dee63bbe4d076c150fbbf_720w.jpg)
>
>   >​		这种被称为“稀疏”或“本地”的非分布式表示在很多方面是效率低下的。首先，随着我们观察的形状数量的增加，表示的维度将会增加。更重要的是，它没有给出关于这些形状之间如何相互关联的任何信息。而这是分布式表示真正的价值所在：它有通过概念来发现数据之间“语义相似性”的能力。
>   >
>   >![img](https://pic3.zhimg.com/80/v2-e514ee19ef54c04a846b0583c955c48a_720w.jpg)
>   >
>   >​		图2显示了同一组形状的分布式表示。它用与方向和形状概念相关的多个“记忆单元”表示形状的信息。“记忆单元”含有关于每个形状以及形状之间如何相互关联的信息。当通过分布式表示法（例如图3中的圆圈）来表示一个新形状时，我们不会再增加维度。而且即使我们之前没有见过圆，我们也知道关于圆的一些信息，因为它与其他形状有关。
>   >
>   >![img](https://pic2.zhimg.com/80/v2-f75ce63a90ee3dc479cb969f0ad44451_720w.jpg)
>   >
>   >​		在上面对于形状的分布式表示的例子里，我们用了四个概念或特征（垂直、水平、矩形、椭圆）来表示。在这种情况下，我们必须事先知道这些重要和显着的特征是什么。但在很多情况下，这是一件很难或不可能的事情。正因为如此，特征工程在经典机器学习技术中才变得如此重要。找到对于数据的良好表示对于分类或聚类等任务的成功至关重要。深度学习能获得巨大成功的原因之一是神经网络具有学习丰富的分布式数据表示的能力。
>   >
>   >​		在一个用LSTM做句子情感分类的任务中，我们通过创建一个映射表来记录句子中每个单词到一个整数索引的关系。单词到整数的映射是单词数据的非分布式稀疏表示。例如，buy这个单词被映射到索引值25，long这个单词被表示为索引68。值得注意的是，这个方法和一个长度为vocab_size的“one-hot编码”（索引中有1表示该单词，而其他位置都是0）向量表示方式是等价的。这两个表示法是相互独立的，尽管在它们在语义上是相似的。两种方法里的两个词之间是没有关系信息的，两种方法里的词都仅被表示为在映射里的位置。
>   >
>   >​		要想得到单词的分布式表示，可以有两种方式。一，是通过Word2Vecotr等模型学习得到。二，作为特定任务的模型训练过程的一部分，用端到端的方式学习来获得分布式表示。
>   >
>   >​		**总结**，神经网络能学习到分布式数据表示的能力是深度学习能对于许多不同类型的问题非常有效的主要原因之一。这个概念的力量和美感使得表示学习成为深度学习研究中最令人兴奋和最活跃的领域之一。
>
>   
>
>2. **word2vec**
>
>   ​		Word2vec 是 Word Embedding方式之一，属于 NLP领域。他是将词转化为「可计算」「结构化」的向量的过程。这是一种基于统计方法来获得词向量的方法，他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。
>
>   ​		这种算法有2种训练模式：
>
>     * 通过上下文来预测当前词（CBOW）
>
>     * 通过当前词来预测上下文（Skip-gram）
>
>       ![CBOW通过上下文来预测当前值](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-09-26-cbow.png)
>
>       ![Skip-gram用当前词来预测上下文](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-09-26-Skip-gram.png)
>
>       优点：
>
>        * 由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）
>        * 比之前的 Embedding方 法维度更少，所以速度更快
>        * 通用性很强，可以用在各种 NLP 任务中
>
>       缺点：
>
>       	* 由于词和向量是一对一的关系，所以多义词的问题无法解决
>       	* Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化
>
>   2.1 Word Embedding
>
>   ​		它就是将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。
>
>   ![word embedding：将非结构化数据转化为结构化数据](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-representation.png)
>
>   ​		文本表示的方法有很多种，下面只介绍 3 类方式：
>
>   		* 独热编码 | one-hot representation
>   		* 整数编码
>   		* 词嵌入 | word embedding
>
>   ![word embedding的关系](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-guanxi.png)
>
>   ​	2.1.1 独热编码
>
>   ![one-hot编码](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-one-hot.png)
>
>   ​		但是在实际情况中，文本中很可能出现成千上万个不同的词，这时候向量就会非常长。其中99%以上都是 0。
>
>   one-hot 的缺点如下：
>
>    - 无法表达词语之间的关系
>
>    - 这种过于稀疏的向量，导致计算和存储的效率都不高
>
>      2.1.2 整数编码
>
>   ![整数编码](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-number.png)
>
>   ​		将句子里的每个词拼起来就是可以表示一句话的向量。
>
>   整数编码的缺点如下：
>
>     - 无法表达词语之间的关系
>
>     - 对于模型解释而言，整数编码可能具有挑战性。
>
>       2.1.3 词嵌入
>
>       词嵌入并不特指某个具体的算法，跟上面2种方式相比，这种方法有几个明显的优势：
>
>       * 他可以将文本通过一个低维向量来表达，不像 one-hot 那么长。
>
>       * 语意相似的词在向量空间上也会比较相近。
>
>       * 通用性很强，可以用在不同的任务中。
>
>         ![word embedding：语意相似的词在向量空间上也会比较相近](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-we.png)
>
>   3. #####  Seq2Seq框架
>
>      ​		NLP应用程序中的大多数基础框架都依赖于序列到序列（seq2seq）模型，在该模型中，不仅输入而且将输出表示为序列。这些模型在各种应用中都很常见，包括**机器翻译、文本摘要、语音到文本和文本到语音**应用。
>
>      ​		最常见的seq2seq框架**由编码器和解码器组成**。**编码器提取输入数据的序列，并生成一个中级输出，随后由解码器消耗，以产生一系列最终输出**。编码器和解码器通常通过一系列递归神经网络或LSTM单元实现。
>
>      ​		
>
>   

### 深度学习中的NLP任务

>1. 词性标注
>
>   1.1  理论
>
>   ​		词性标注在本质上是分类问题，将语料库中的单词按词性分类。一个词的词性由其在所属语言的含义、形态和语法功能决定。以汉语为例，汉语的词类系统有18个子类，包括7类体词，4类谓词、5类虚词、代词和感叹词。词类不是闭合集，而是有兼词现象，例如“制服”在作为“服装”和作为“动作”时会被归入不同的词类，因此词性标注与上下文有关
>
>   1.2  应用
>
>   ​		词性标注是文本数据的**预处理**环节之一，原始文本在NLP或文本挖掘应用中，首先通过字符分割（word segmentation）和字符嵌入（word embedding）被**向量化**，随后通过词性标注得到高阶层特征，并输入[语法分析器](https://baike.baidu.com/item/语法分析器/10598664)执行[语义分析](https://baike.baidu.com/item/语义分析/8853372)（sentiment analysis）、[指代消解](https://baike.baidu.com/item/指代消解/7605552)（coreference resolution）等任务
>
>   1.2.1  文本向量化
>
>   ​		文本表示是自然语言处理中的基础工作，文本表示的好坏直接影响到整个自然语言处理系统的性能。文本向量化就是**将文本表示成一系列能够表达文本语义的向量**，是文本表示的一种重要方式。目前对文本向量化大部分的研究都是通过**词向量化**实现的，也有一部分研究者将句子作为文本处理的基本单元，于是产生了doc2vec和str2vec技术。
>
>     * 词袋模型
>        
>       ​		最早的以词语为基础处理单元的文本项量化方法。该模型产生的向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频率。
>        
>     * 词向量(word2vec)  
>        
>       ​		随着互联网技术的发展，大量无标注数据的产生，研究重心转移到利用无标注数据挖掘有价值的信息上来。词向量(word2vec)技术就是为了利用神经网络，从大量无标注的文本中提取有用的信息而产生的。
>        
>       ​		相比于词袋模型，词向量是一种更为有效的表征方式。怎么理解呢？词向量其实就是用一个一定维度(例如128，256维)的向量来表示词典里的词。
>        
>       经过训练之后的词向量，能够表征词语之间的关系。例如，“香蕉”和“苹果”之间的距离，会比“香蕉”和“茄子”之间的距离要近。
>        
>       通过多维向量表示，也能更为方便的进行计算。例如，“女人”+“漂亮” =“女神”。
>
>   * 神经概率语言模型
>
>     ​		一个语言模型通常构建为一句话的概率分布p(W)，这里的p(W)实际上反映的是W作为一个句子出现的概率。 说成大白话，**语言模型就是计算某个句子出现的概率**。
>
>     ​		一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。我们在求P(W)时实际上就已经建立了一个模型，这里的诸多条件概率就是模型的参数。如果能够通过语料，将这些参数已学习到，就能够计算出一个句子出现的概率。
>
>2. 句法分析
>
>3. 
>
>